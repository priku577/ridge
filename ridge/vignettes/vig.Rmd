---
title: "ridgereg"
author: "Boxi Zhang"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r}
library(ridge)
library(mlbench)
library(caret) 
library(leaps)
data("BostonHousing")
```



# Introduction
In this vignette, it will be shown how to do a simple prediction problem using ridgereg() function in the package "mlr" with the data _BostonHousing_ found in the mlbench package. The dataset consists of 506 observations of 14 variables. The median value of house price in $1000s, denoted by MEDV, is the outcome or the target variable in our model. Below is a brief description of each feature and the outcome in our dataset:

1. CRIM \-\-\- per capita crime rate by town
2. ZN \-\-\- proportion of residential land zoned for lots over 25,000 sq.ft
3. INDUS \-\-\- proportion of non-retail business acres per town
4. CHAS \-\-\- Charles River dummy variable (1 if tract bounds river; else 0)
5. NOX \-\-\- nitric oxides concentration (parts per 10 million)
6. RM \-\-\- average number of rooms per dwelling
7. AGE \-\-\- proportion of owner-occupied units built prior to 1940
8. DIS \-\-\- weighted distances to five Boston employment centres
9. RAD \-\-\- index of accessibility to radial highways
10. TAX \-\-\- full-value property-tax rate per $10,000
11. PTRATIO \-\-\- pupil-teacher ratio by town
12. B \-\-\- 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town
13. LSTAT \-\-\- % lower status of the population
14. MEDV \-\-\- Median value of owner-occupied homes in $1000â€™s





###1. Divide the BostonHousing data into a test and training dataset

```{r}
data("BostonHousing")
names(BostonHousing)
train_index <- caret::createDataPartition(BostonHousing$age, p = .75,
                                         list = FALSE,
                                         times= 1)
train_data <- BostonHousing[train_index, ]
test_data <- BostonHousing[-train_index, ]


head(train_data)
head(test_data)
```

##2. Fit a linear regression model and with a linear regression model with forward selection of covariates on the training datasets.

Fit a linear regression model:

```{r}
ridge <- caret::train(crim~.,
                      data = train_data,
                      method='lm',
                      trControl = trainControl(method = "cv")
)

print(ridge)
```

fit a linear model with method=leapForward
```{r}
lflmGrid <- expand.grid(nvmax=1:(ncol(train_data)-1))

ridge <- caret::train(crim~.,
                      data = train_data,
                      method='leapForward',
                      tuneGrid = lflmGrid
)
print(ridge)
```

##3. Evaluate the performance of this model on the training dataset.

Since the RMSE & MAE is low on training of lm model compared to leapForward lm where model has good perfomance with nvmax(number of predictors) we can conclude that LM is better than leapforward LM.

##4. Fit a ridge regression model using your ridgereg() function to the training dataset for different values of lambda.


```{r}
ridge <- list(type="Regression", 
              library="ridge",
              loop=NULL,
              prob=NULL)

ridge$parameters <- data.frame(parameter="lambda",
                               class="numeric",
                               label="lambda")


ridge$grid <- function (x, y, len = NULL, search = "grid"){
  data.frame(lambda = lambda)
} 

ridge$fit <- function (x, y, wts, param, lev, last, classProbs, ...) {
  dat <- if (is.data.frame(x)) 
    x
  else as.data.frame(x)
  dat$.outcome <- y
  out <- ridgereg$new(.outcome ~ ., data=dat ,lambda = param$lambda, normalize=normalize, ...)
  
  out
}

ridge$predict <- function (modelFit, newdata, submodels = NULL) {
  if (!is.data.frame(newdata)) 
    newdata <- as.data.frame(newdata)
  newdata <- scale(newdata)
  modelFit$predict(newdata)
}
```

##5. Find the best hyperparameter value for lambda using 10-fold cross-validation on the training set.

```{r,eval=FALSE}
library(MASS)
fitControl <- caret::trainControl(method = "cv",
                                  number = 10)
lambdaGrid <- expand.grid(lambda = c(0,.01,.02,.03,.04))
ridge <- caret::train(crim~.,
                      data = train_data,
                      method='ridge',
                      trControl = fitControl,
                      tuneGrid = lambdaGrid,
                      preProcess=c('center', 'scale')
)
predict(ridge$finalModel, type='coef', mode='norm')$coefficients[13,]
ridge.pred <- predict(ridge, test_data)
avgErrror<-2*sqrt(mean(ridge.pred - test_data$crim)^2)
print(ridge)
```

So, the best hyperparameter value for lambda is 0.03


##6. Evaluate the performance of all three models on the test dataset.

By evaluating three models of

Linear Regression
Linear Regression with leapForward
Ridge Regression

Ridge regression on training set with best value of lambda gives lower RMSE.
